{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pokédex en la vida real con IA",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RodolfoFerro/real-pokedex-ia/blob/main/notebooks/Pok%C3%A9dex%20en%20la%20vida%20real%20con%20IA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6jO_1gISKxk"
      },
      "source": [
        "# Pokédex en la vida real con IA\n",
        "\n",
        "> **Rodolfo Ferro** <br>\n",
        "> Google Dev Expert en ML, 2021.\n",
        "\n",
        "## Contenido\n",
        "\n",
        "1. Contexto general sobre la IA\n",
        "    - Introducción al aprendizaje de máquina\n",
        "    - Conceptos básicos: Conjunto de entrenamiento, conjunto de prueba\n",
        "2. Redes neuronales artificiales:\n",
        "    - Perceptrón\n",
        "    - Perceptrón multicapa y redes profundas\n",
        "    - Redes neuronales convolucionales\n",
        "3. Clasificador de animales"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPk1Rkc4FZ5g"
      },
      "source": [
        "### **Historia de las redes neuronales**\n",
        "\n",
        "La historia de las redes neuronales se remontan a un tipo de neurona artificial, llamada **perceptrón**. Estos fueron desarrollados entre 1950 y 1960 por el científico **Frank Rosenblatt**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uehq48zoSocy"
      },
      "source": [
        "\n",
        "\n",
        "### **Entonces, ¿qué es un perceptrón?**\n",
        "\n",
        "Un perceptrón es una abstracción de una neurona real.\n",
        "\n",
        "Éste toma varias **entradas** $x_1, x_2,..., x_n $ y produce una **salida**. Para la salida, Rosenblatt propuso que las entradas tuviesen **pesos** asciados $w_1, w_2, ..., w_n$, siendo estos números reales que expresan la importancia respectiva de cada entrada para la salida. La salida de la neurona, $0$ o $1$, está determinada con base en que la suma ponderada, \n",
        "\n",
        "$$\\displaystyle\\sum_{j}w_jx_j,$$\n",
        "\n",
        "<!-- $\\textbf{w}_{Layer}\\cdot\\textbf{x} = \n",
        "\\begin{bmatrix}\n",
        "w_{1, 1} & w_{1, 2} & \\cdots & w_{1, n}\\\\\n",
        "w_{2, 1} & w_{2, 2} & \\cdots & w_{2, n}\\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
        "w_{m, 1} & w_{m, 2} & \\cdots & w_{m, n}\\\\\n",
        "\\end{bmatrix} \\cdot\n",
        "\\begin{bmatrix}\n",
        "x_1\\\\\n",
        "x_2\\\\\n",
        "\\vdots\\\\\n",
        "x_n\n",
        "\\end{bmatrix}$ -->\n",
        "\n",
        "(para $j \\in \\{1, 2, ..., n\\}$ ) sea menor o mayor que un **valor límite** que por ahora llamaremos umbral.\n",
        "\n",
        "Resumiendo, un perceptron es un sistema que toma decisiones con base en la evidencia presentada.\n",
        "\n",
        "<center>\n",
        "    <img width=\"50%\" src=\"https://camo.githubusercontent.com/0e433317a51ea67fb061925026ed3c1c3692cb35/68747470733a2f2f696e7369676874732e7365692e636d752e6564752f7365695f626c6f672f73657374696c6c695f646565706c6561726e696e675f6172746966696369616c6e6575726f6e332e706e67\">\n",
        "</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q33kCpXyFgJ_"
      },
      "source": [
        "#### **Hagamos un ejemplo**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cLBMuek3lBHd"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "# Primero creamos nuestra clase perceptron\n",
        "class perceptron():\n",
        "    def __init__(self, inputs, weights):\n",
        "        self.inputs = np.array(inputs)\n",
        "        # TODO: Convierte los pesos a arreglo de NumPy como arriba\n",
        "        self.weights = None\n",
        "  \n",
        "    def decide(self, treshold):\n",
        "        return None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t42O74IdmKIw"
      },
      "source": [
        "# Ahora necesitamos darle sus entradas y pesos asociados\n",
        "inputs, weights = [], []\n",
        "\n",
        "preguntas = [\n",
        "    \"· ¿Cuál es la velocidad? \",\n",
        "    \"· ¿Ritmo cardiaco? \",\n",
        "    \"· ¿Respiración? \"\n",
        "]\n",
        "\n",
        "for pregunta in preguntas:\n",
        "    i = int(input(pregunta))\n",
        "    w = int(input(\"· Y su peso asociado es... \"))\n",
        "    inputs.append(i)\n",
        "    weights.append(w)\n",
        "    print()\n",
        "\n",
        "treshold = int(input(\"· Y nuestro umbral/límite será: \"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZHjy-k33oNFm"
      },
      "source": [
        "p = perceptron() # TODO Instancía un objeto perceptron con entradas y pesos \n",
        " # TODO Aplica la función de decisión con el umbral"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUCCwUG6DgCX"
      },
      "source": [
        "### **Bias y funciones de activación**\n",
        "\n",
        "_Antes de seguir, introduciremos otro concepto, que es el **bias**._\n",
        "\n",
        "La operación matemática que realiza la neurona se puede escribir como:\n",
        "\n",
        "$$ f(\\textbf{x}) = \n",
        "  \\begin{cases}\n",
        "    0 & \\text{si $\\displaystyle\\sum_{j}w_jx_j <$ valor límite o treshold} \\\\\n",
        "    1 & \\text{si $\\displaystyle\\sum_{j}w_jx_j \\geq$ valor límite o treshold} \\\\\n",
        "  \\end{cases},$$\n",
        "\n",
        "donde $\\textbf{x} = (x_1, x_2, ..., x_n)$ y $j \\in \\{1, 2, ..., n\\}$.\n",
        "\n",
        "De lo anterior, podemos despejar el valor límite (el umbral) y escribirlo como $b$, obteniendo:\n",
        "\n",
        "$$ f(\\textbf{x}) = \n",
        "  \\begin{cases}\n",
        "    0 & \\text{si $\\displaystyle\\sum_{j}w_jx_j + b < 0$} \\\\\n",
        "    1 & \\text{si $\\displaystyle\\sum_{j}w_jx_j + b > 0$} \\\\\n",
        "  \\end{cases},$$\n",
        "\n",
        "donde $\\textbf{x} = (x_1, x_2, ..., x_n)$ y $j \\in \\{1, 2, ..., n\\}$.\n",
        "\n",
        "Esto que escribimos como $b$, también se le conoce como **bias**, y describe *qué tan susceptible la red es a __dispararse__*.\n",
        "\n",
        "Curiosamente, esta descripción matemática encaja con la función de salto, que es una función de activación. Esto es, una función que permite el paso de información de acuerdo a la entrada y los pesos, permitiendo el disparo del lo procesado hacia la salida. La función de salto se ve como sigue:\n",
        "\n",
        "<center>\n",
        "    <img src=\"https://upload.wikimedia.org/wikipedia/commons/4/4a/Funci%C3%B3n_Cu_H.svg\" width=\"40%\" alt=\"Función escalón de Heaviside\">\n",
        "</center>\n",
        "\n",
        "Sin embargo, podemos hacer a una neurona aún más susceptible con respecto a los datos de la misma (entradas, pesos, bias) añadiendo una función sigmoide. La función sigmoide se ve como a continuación: \n",
        "\n",
        "<center>\n",
        "    <img src=\"https://upload.wikimedia.org/wikipedia/commons/6/66/Funci%C3%B3n_sigmoide_01.svg\" width=\"40%\" alt=\"Función sigmoide\">\n",
        "</center>\n",
        "\n",
        "Esta función es suave, y por lo tanto tiene una diferente \"sensibililad\" a los cambios abruptos de valores. También, sus entradas en lugar de solo ser $1$'s o $0$'s, pueden ser valores en todos los números reales. La función sigmoide es descrita por la siguiente expresión matemática:\n",
        "\n",
        "$$f(z) = \\dfrac{1}{1+e^{-z}}$$\n",
        "\n",
        "O escrito en términos de pesos y biases:\n",
        "\n",
        "$$f(z) = \\dfrac{1}{1+\\exp{\\left\\{-\\left(\\displaystyle\\sum_{j}w_jx_j +b\\right)\\right\\}}}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0G1MY4HQFsEd"
      },
      "source": [
        "#### **Volviendo al ejemplo**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qSn8VaEoDtHo"
      },
      "source": [
        "# Modificamos para añadir la función de activación\n",
        "class SigmoidNeuron():\n",
        "    def __init__(self, inputs, weights):\n",
        "        self.inputs = np.array(inputs)\n",
        "        self.weights = np.array(weights)\n",
        "  \n",
        "    def decide(self, bias):\n",
        "        z = (self.weights @ self.inputs) + bias\n",
        "        return 1. / (1. + np.exp(-z))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ogPy6NpfERfJ"
      },
      "source": [
        "bias = int(input(\"· El nuevo bias será: \"))\n",
        "s = None # TODO Instantiate SigmoidNeuron\n",
        "s.decide(bias)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mRGlbVZsFxdk"
      },
      "source": [
        "> Esta es la neurona que usaremos para los siguientes tópicos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NvmIk2G9EgOQ"
      },
      "source": [
        "<center>\n",
        "    ******************\n",
        "</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YoXTAmOrjmPP"
      },
      "source": [
        "### **¿Cómo funciona el entrenaiento?**\n",
        "\n",
        "A lo largo de este sección explicaré cómo una sola neurona puede tomar una decisión sencila. \n",
        "\n",
        "Para este problema construiremos un perceptrón simple, como propusieron [McCulloch & Pitts](https://es.wikipedia.org/wiki/Neurona_de_McCulloch-Pitts) en los 90's y usando la [función sigmoide](https://en.wikipedia.org/wiki/Sigmoid_function).\n",
        "\n",
        "\n",
        "### Problema:\n",
        "\n",
        "Queremos mostrarle a una neurona un conjunto de ejemplos para que pueda aprender cómo se comportan los datos y pueda asignar una función. El conjunto de ejemplos es el siguiente:\n",
        "\n",
        "- La entrada `(1, 0)` debe tener como salida `1`.\n",
        "- La entrada `(0, 1)` debe tener como salida `1`.\n",
        "- La entrada `(0, 0)` debe tener como salida `0`.\n",
        "\n",
        "Así, si damos como entrada a la neurona el valor `(1, 1)`, ésta debería se capaz de predecir el valor `1`.\n",
        "\n",
        "##### (¿Puedes adivinar qué función es? _HINT: Es una compuerta lógica._)\n",
        "\n",
        "> #### ¿Qué necesitamos hacer?\n",
        "> Programar y entrenar una neurona para realizar predicciones.\n",
        ">\n",
        "> Específicamente, haremos lo siguiente:\n",
        "> - Programar una clase y su constructor\n",
        "> - Definir la función sirgmoide y su derivada\n",
        "> - Definir el número de épocas de entrenamiento\n",
        "> - Resolver el problema y predecir el valor de la entrada deseada"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xnHyOcf2jnB2"
      },
      "source": [
        "class neurona_sigmoide():\n",
        "    def __init__(self, n):\n",
        "        \"\"\"Constructor of the class.\"\"\"\n",
        "        np.random.seed(123)\n",
        "        self.pesos_sinapticos = 2 * np.random.random((n, 1)) - 1\n",
        "\n",
        "    def sigmoide(self, x):\n",
        "        \"\"\"Sigmoid function.\"\"\"\n",
        "        # TODO.\n",
        "        pass\n",
        "\n",
        "    def derivada_sigmoide(self, x):\n",
        "        \"\"\"Derivative of the Sigmoid function.\"\"\"\n",
        "        # TODO.\n",
        "        pass\n",
        "\n",
        "    def entrena(self, entradas_ejemplo, salidas_ejemplo, epocas):\n",
        "        \"\"\"Training function.\"\"\"\n",
        "        for epoca in range(epocas):\n",
        "            salida = self.predice(entradas_ejemplo)\n",
        "            error = salidas_ejemplo.reshape((len(entradas_ejemplo), 1)) - salida\n",
        "            ajuste = np.dot(entradas_ejemplo.T, error * self.derivada_sigmoide(salida))\n",
        "            self.pesos_sinapticos += ajuste\n",
        "\n",
        "    def predice(self, entradas):\n",
        "        \"\"\"Prediction function.\"\"\"\n",
        "        return self.sigmoide(np.dot(entradas, self.pesos_sinapticos))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hUL-pwJlmbtG"
      },
      "source": [
        "#### **Generando los ejemplos**\n",
        "\n",
        "Ahora generamos una lista con los ejemplos basándonos en la descripción del problema."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f48ENlcjnEc0"
      },
      "source": [
        "entradas = [] # TODO. Define los valores de entrada.\n",
        "salidas = []  # TODO. Define las salidas de cada entrada.\n",
        "\n",
        "entradas_ejemplo = np.array(entradas)\n",
        "salidas_ejemplo = np.array(salidas).T.reshape((3, 1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWrw6XQJm3by"
      },
      "source": [
        "#### **Entrenando la neurona**\n",
        "\n",
        "Para realizar el entrenamiento, primero crearemos una neurona. Por default contendrá pesos aleatorios (spues aún no ha sido entrenada con los ejemplos):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CrQOgIBToNz2"
      },
      "source": [
        "neurona = neurona_sigmoide(2)\n",
        "print(\"Pesos sinápticos inicialmente aleatorios:\")\n",
        "neurona.pesos_sinapticos"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xVwxZVJnN3O"
      },
      "source": [
        "Ahora vamos a entrenar la neurona una cantidad de épocas definida y vamos a ver cómo cambian los pesos sinápticos:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3fXtP94nZTp"
      },
      "source": [
        "# TODO. Podemos modificar el número de épocas para mejorar el aprendizaje.\n",
        "epocas = 0\n",
        "\n",
        "# Entrenamos la neurona\n",
        "neurona.entrena(entradas_ejemplo, salidas_ejemplo, epocas)\n",
        "print(\"Nuevos pesos sináptico después del entrenamiento: \")\n",
        "neurona.pesos_sinapticos"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQsTx9bmnwE4"
      },
      "source": [
        "#### **Realizando predicciones:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ai142AuNnuyA"
      },
      "source": [
        "uno_uno = np.array((1, 1))\n",
        "print(\"Predicción para (1, 1): \")\n",
        "neurona.predice(uno_uno)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Km7Ow80drf2"
      },
      "source": [
        "### **Redes de neuronas artificiales**\n",
        "\n",
        "Imaginemos que en lugar de una única neurona, tenemos 7 neuronas, todas con diferentes pesos (que inicialmente definiremos de manera aleatoria), dispuestas con la siguiente configuración: \n",
        "- Tenemos dos neuronas que representarán la información de entrada\n",
        "- Cada una de las dos neuronas está conectada con cuatro siguientes neuronas dispuestas en una siguiente capa\n",
        "- Finalmente, cada salida de las cuatro neuronas previas es conectada a una única neurona dispuesta en una capa de salida\n",
        "\n",
        "La configuración quedaría como se muestra a continuación:\n",
        "\n",
        "<center>\n",
        "    <img src=\"https://www.pngitem.com/pimgs/m/531-5314899_artificial-neural-network-png-transparent-png.png\" width=\"30%\" alt=\"A simple neural network\">\n",
        "</center>\n",
        "\n",
        "La previa configuración de neuronas dispuesta en forma de red podría tomar decisiones más complejas y abstractas, si consideramos la operación definida anteriormente (pesos, bias, función de activación) sobre los pesos de cada conexión de neuronas entre las capas.\n",
        "\n",
        "Esto, a grandes rasgos, es una manera de abstraer una red neuronal artifical, compuesta por neuronas como previamente hemos definido; y donde se tienen capas de neuronas cuyas salidas funcionan como las entradas de otras neuronas (en las siguientes capas).\n",
        "\n",
        "Con esto en mente, podemos programar redes neuronales artificiales más complejas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fm1C_NXdrUOa"
      },
      "source": [
        "#### **TensorFlow**\n",
        "\n",
        "TensorFlow es un paquete de Googloe diseñado para programar, entrenar y desplegar modelos de inteligencia artificial basados en redes nueronales artificiales.\n",
        "\n",
        "Este paquete ya incluye diferentes capas de neuronas y funciones de procesamiento para los datos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LawUXKYNrXNI"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "print(f\"Tensorflow version: {tf.__version__}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RK5aWo80rxlA"
      },
      "source": [
        "#### **Descarga de datos**\n",
        "\n",
        "Procederemos a descargar los datos del conjunto \"Animals-10: Animal pictures of 10 different categories taken from google images\" que se encuentra disponible en Kaggle (https://www.kaggle.com/alessiocorrado99/animals10), los cuales he respaldado en Google Drive por si no tienes una cuenta de Kaggle.\n",
        "\n",
        "Podemos descargar y descomprimir los datos con el código a continuación."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-ZlDg_IvYdl"
      },
      "source": [
        "!curl -L -c cookies.txt 'https://docs.google.com/uc?export=download&id=1bvZrHMjucy8ZgFH6hktmDiQ5NeroPxGw' | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1/p' > confirm.txt\n",
        "!curl -L -b cookies.txt -o dataset.zip 'https://docs.google.com/uc?export=download&id=1bvZrHMjucy8ZgFH6hktmDiQ5NeroPxGw&confirm='$(<confirm.txt)\n",
        "!rm -f confirm.txt cookies.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UfLcRF3BNMY6"
      },
      "source": [
        "!unzip dataset.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvhWHtsmN3XW"
      },
      "source": [
        "<center>\n",
        "    ********** EXPERIMENTAL **********\n",
        "</center>\n",
        "\n",
        "Borraré algunas imágenes para que el cuaderno soporte ejecutar el modelo en la nube...\n",
        "\n",
        "Al final nos quedaremos con 5k imágenes para entrenar."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DXZz54bnOCmK"
      },
      "source": [
        "from glob import glob\n",
        "from os import remove\n",
        "\n",
        "folders = glob('raw-img/*')\n",
        "for folder in folders:\n",
        "    files = glob(folder + '/*')\n",
        "    for filename in files[500:]:\n",
        "        remove(filename)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rAlArGWjPWr_"
      },
      "source": [
        "Comprobamos el toal de archivos por clase."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vzYThc4wPY8L"
      },
      "source": [
        "folders = glob('raw-img/*')\n",
        "for folder in folders:\n",
        "    files = glob(folder + '/*')\n",
        "    print(f'Folder {folder}: {len(files)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AK3xmUnLvNwQ"
      },
      "source": [
        "#### **Carga de datos**\n",
        "\n",
        "Crearemos variables para entrenamiento y pruebas de los datos originales."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PvwP-nxRrbTT"
      },
      "source": [
        "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
        "\n",
        "\n",
        "# MetayA\n",
        "BATCH_SIZE = 32\n",
        "IMG_SIZE = 180\n",
        "\n",
        "# Conjuntos de entrenamiento y prueba\n",
        "train_ds = image_dataset_from_directory(directory='raw-img', \n",
        "                                        validation_split=0.2,\n",
        "                                        subset='training',\n",
        "                                        labels='inferred', \n",
        "                                        label_mode='categorical',\n",
        "                                        seed=123,\n",
        "                                        image_size=(IMG_SIZE, IMG_SIZE),\n",
        "                                        batch_size=BATCH_SIZE)\n",
        "\n",
        "val_ds = image_dataset_from_directory(directory='raw-img', \n",
        "                                        validation_split=0.2,\n",
        "                                        subset='validation',\n",
        "                                        labels='inferred', \n",
        "                                        label_mode='categorical',\n",
        "                                        seed=123,\n",
        "                                        image_size=(IMG_SIZE, IMG_SIZE),\n",
        "                                        batch_size=BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T75eXJeFzofs"
      },
      "source": [
        "Podemos identificar las clases asociadas a cada elemento."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mTlQbfxpyp-5"
      },
      "source": [
        "# Classes originales\n",
        "class_names = train_ds.class_names\n",
        "print(class_names)\n",
        "\n",
        "# Diccionario de valores\n",
        "translate = {\n",
        "    \"cane\": \"dog\",\n",
        "    \"cavallo\": \"horse\",\n",
        "    \"elefante\": \"elephant\",\n",
        "    \"farfalla\": \"butterfly\",\n",
        "    \"gallina\": \"chicken\",\n",
        "    \"gatto\": \"cat\",\n",
        "    \"mucca\": \"cow\",\n",
        "    \"pecora\": \"sheep\",\n",
        "    \"scoiattolo\": \"squirrel\",\n",
        "    \"dog\": \"cane\",\n",
        "    \"cavallo\": \"horse\",\n",
        "    \"elephant\" : \"elefante\",\n",
        "    \"butterfly\": \"farfalla\",\n",
        "    \"chicken\": \"gallina\",\n",
        "    \"cat\": \"gatto\",\n",
        "    \"cow\": \"mucca\",\n",
        "    \"spider\":\n",
        "    \"ragno\",\n",
        "    \"squirrel\": \"scoiattolo\"\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ckY-uU0zr9N"
      },
      "source": [
        "Y de igual manera, explorar algunas imágenes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nS38xN6JzbB1"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=[10,10])\n",
        "for image_batch, label_batch in train_ds.take(1):\n",
        "    for i in range(9):\n",
        "        ax = plt.subplot(3, 3, i+1)\n",
        "        plt.imshow(image_batch[i*3].numpy().astype('uint8'))\n",
        "        plt.axis(\"off\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qlRa_Cmm0PHh"
      },
      "source": [
        "Al final, las imágenes son tensores, así que podemos ver su tamaño."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwNemstx0S68"
      },
      "source": [
        "image_batch.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-9PU2rZ1hWE"
      },
      "source": [
        "##### **Configuración del dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k8VB_LL61iQJ"
      },
      "source": [
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "train_ds = train_ds.cache().shuffle(1024).prefetch(buffer_size=AUTOTUNE)\n",
        "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8NX8liFk0SlS"
      },
      "source": [
        "#### **Aumento de datos**\n",
        "\n",
        "En IA existe una técnica llamada _data augmentation_, que consiste en aumentar los datos para diversificar el dataset al rotar, recortar o espejear imágenes.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n72s4pKV1k38"
      },
      "source": [
        "data_augmentation = tf.keras.Sequential(\n",
        "    [\n",
        "        tf.keras.layers.experimental.preprocessing.RandomFlip(mode='horizontal', input_shape=(IMG_SIZE, IMG_SIZE, 3)),\n",
        "        tf.keras.layers.experimental.preprocessing.RandomRotation(factor=0.2),\n",
        "        tf.keras.layers.experimental.preprocessing.RandomContrast(0.1),\n",
        "        tf.keras.layers.experimental.preprocessing.RandomZoom(0.1),\n",
        "    ],\n",
        "    name=\"data_augmentation\"\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6tPz9uH8By3m"
      },
      "source": [
        "Podemos explorar los resultados del aumento de datos..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_K3nboJpB3fM"
      },
      "source": [
        "plt.figure(figsize=[10,10])\n",
        "for image_batch, _ in train_ds.take(1):\n",
        "    for i in range(9):\n",
        "        augmented_images = data_augmentation(image_batch)\n",
        "        ax = plt.subplot(3, 3, i+1)\n",
        "        plt.imshow(augmented_images[1].numpy().astype('uint8'))\n",
        "        plt.axis(\"off\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FoqX_M4SDE6y"
      },
      "source": [
        "### **Pokédex en la vida real**\n",
        "\n",
        "Ahora podemos proceder a estructurar el modelo de IA, entrenarlo y usarlo para que sea capaz de identificar animales.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBGZ9oMmDV7m"
      },
      "source": [
        "#### **El modelo: Una CNN**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B56CInMzDvQW"
      },
      "source": [
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import MaxPool2D\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dense\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "tf.keras.backend.clear_session()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkJhTBR3EgIP"
      },
      "source": [
        "Vamos a crear una red neuronal convolucional, capaz de procesar imágenes y de identificar lo que hay en las mismas. Para ello, utilizaremos TensorFlow, lo que nos simplificará la labor de una manera muy impresionante."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96w6ejPYEsxn"
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "    # Si realizamos aumento de datos, ésta debería ser la entrada\n",
        "    # data_augmentation,\n",
        "    # tf.keras.layers.experimental.preprocessing.Rescaling(scale=1./255),\n",
        "\n",
        "    # Podemos agregar más preprocesamientos necesarios\n",
        "    tf.keras.layers.experimental.preprocessing.Rescaling(scale=1./255, input_shape=(IMG_SIZE, IMG_SIZE, 3)),\n",
        "\n",
        "    # Procedemos a agregar capas convolucionales y de escalado\n",
        "    Conv2D(filters=16, kernel_size=3, padding='same', activation='relu'),\n",
        "    MaxPool2D(),\n",
        "\n",
        "    Conv2D(64, 3, padding='same', activation='relu'),\n",
        "    MaxPool2D(),\n",
        "    # Puedes agregar más capas convolucionales...\n",
        "\n",
        "    # Evitamos overfitting\n",
        "    Dropout(0.2),\n",
        "\n",
        "    # Pasamos de feature maps a entrdas para capas densas\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "\n",
        "    # Concluimos con una capa densa de acuerdo al número de clases\n",
        "    Dense(10),\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j58mUCD1GM-T"
      },
      "source": [
        "Definimos la función de pérdida (métrica) y compilamos el modelo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vAavtKRvF96g"
      },
      "source": [
        "model.compile(\n",
        "    loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9rmQg0TGT5D"
      },
      "source": [
        "Definimos épocas y procedemos a entrenar el modelo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MUwtcs5FGXER"
      },
      "source": [
        "EPOCHS = 5\n",
        "\n",
        "history = model.fit(train_ds, validation_data=val_ds, epochs=EPOCHS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Zvg03LrHxWe"
      },
      "source": [
        "#### **Evaluación del modelo**\n",
        "\n",
        "Cuando entrenamos modelos de IA, siempre es bueno tener un marco de referencia para saber qué tan bueno (o no) es nuestro modelo. La idea de utilizar un dataset para prueba y para validación tiene como principal objetivo esta parte."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qu8WwrVZIEUC"
      },
      "source": [
        "plt.style.use('seaborn')\n",
        "\n",
        "\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs_range = range(EPOCHS)\n",
        "\n",
        "plt.figure(figsize=(20, 8))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
        "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Training Loss')\n",
        "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgpkGX_8JmM7"
      },
      "source": [
        "#### **Clasificación de especies**\n",
        "\n",
        "Una vez que entrenamos el modelo y que los pesos se ajustan, podemos proceder a utilizarlo para identificar especies.\n",
        "\n",
        "Cargamos una imagen aleatoria de la carpeta."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mIO31K3rJxXx"
      },
      "source": [
        "import random\n",
        "import os\n",
        "\n",
        "\n",
        "dir_path = 'raw-img/'\n",
        "class_label = random.choice(os.listdir(\"raw-img/\"))\n",
        "print(\"Class Label chosen: \", class_label)\n",
        "file = random.choice(os.listdir(\"raw-img/\" + class_label))\n",
        "print(file)\n",
        "\n",
        "file_path = dir_path + class_label + \"/\" + file\n",
        "print(file_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVvJOK3zRCt9"
      },
      "source": [
        "Mostramos la imagen."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M9HaLHOPQsrn"
      },
      "source": [
        "img = plt.imread(file_path)\n",
        "plt.imshow(img)\n",
        "plt.grid(False)\n",
        "print(\"Clase original:\", translate[class_label])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HgNDlNa4REOQ"
      },
      "source": [
        "Procedemos a predecir."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zwlI5kOfQ9Mm"
      },
      "source": [
        "img = tf.keras.preprocessing.image.load_img(file_path, target_size=(IMG_SIZE, IMG_SIZE, 3))\n",
        "\n",
        "img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
        "img_array = tf.expand_dims(img_array, 0)\n",
        "\n",
        "predictions = model.predict(img_array)\n",
        "score = tf.nn.softmax(predictions[0])\n",
        "\n",
        "print(score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGVpYnDGRL6-"
      },
      "source": [
        "predicted_class = class_names[np.argmax(score)]\n",
        "predicted_class_translated = translate[predicted_class]\n",
        "print(f'La clase predicha es \"{predicted_class_translated}\" con un porcentaje {np.max(score)*100:.2f}% de probabilidad.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chtWtf6EwKwP"
      },
      "source": [
        "### **Otros datasets para explorar**\n",
        "\n",
        "- 10 Monkey Species: https://www.kaggle.com/slothkong/10-monkey-species\n",
        "- Cat Dataset: https://www.kaggle.com/crawford/cat-dataset\n",
        "- Stanford Dogs Dataset: https://www.kaggle.com/jessicali9530/stanford-dogs-dataset\n",
        "\n",
        "### **Referencias útiles sobre los contenidos**\n",
        "\n",
        "- Animal classification: https://www.kaggle.com/rahulkod/animal-classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qrNQq5k9QZrm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}